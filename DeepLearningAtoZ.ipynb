{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ektGPXSCT_c8",
        "zi8g-PdOU6PT",
        "N2x5chz6Vik2",
        "IaUPwqE9WGAx",
        "X9gHXzrpXHUm",
        "X2ThSy_sYBVr"
      ],
      "authorship_tag": "ABX9TyPpH3TD2E74NYX2gjZyctZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Social-merit/Machine-learning-Deep-learning/blob/main/DeepLearningAtoZ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PubzjSbdR2ph"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Neural Network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "***Artificial Neural Networks (ANN)***\n",
        "*   Additional Reading:\n",
        "\n",
        "    Yann LeCun et al., 1998, [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
        "\n",
        "    By Xavier Glorot et al., 2011 [Deep sparse rectifier neural networks](http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)\n",
        "\n",
        "    CrossValidated, 2015, [A list of cost functions used in neural networks, alongside applications](http://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)\n",
        "\n",
        "    Andrew Trask, 2015, [A Neural Network in 13 lines of Python (Part 2 – Gradient Descent)](https://iamtrask.github.io/2015/07/27/python-network-part2/)\n",
        "\n",
        "    Michael Nielsen, 2015, [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ektGPXSCT_c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Artificial Neural Network\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('Churn_Modelling.csv')\n",
        "X = dataset.iloc[:, 3:-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "# Encoding categorical data\n",
        "# Label Encoding the \"Gender\" column\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "X[:, 2] = le.fit_transform(X[:, 2])\n",
        "print(X)\n",
        "# One Hot Encoding the \"Geography\" column\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))\n",
        "print(X)\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Part 2 - Building the ANN\n",
        "\n",
        "# Initializing the ANN\n",
        "ann = tf.keras.models.Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Part 3 - Training the ANN\n",
        "\n",
        "# Compiling the ANN\n",
        "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Training the ANN on the Training set\n",
        "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)\n",
        "\n",
        "# Part 4 - Making the predictions and evaluating the model\n",
        "\n",
        "# Predicting the result of a single observation\n",
        "\n",
        "\"\"\"\n",
        "Homework:\n",
        "Use our ANN model to predict if the customer with the following informations will leave the bank: \n",
        "Geography: France\n",
        "Credit Score: 600\n",
        "Gender: Male\n",
        "Age: 40 years old\n",
        "Tenure: 3 years\n",
        "Balance: $ 60000\n",
        "Number of Products: 2\n",
        "Does this customer have a credit card? Yes\n",
        "Is this customer an Active Member: Yes\n",
        "Estimated Salary: $ 50000\n",
        "So, should we say goodbye to that customer?\n",
        "\n",
        "Solution:\n",
        "\"\"\"\n",
        "\n",
        "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)\n",
        "\n",
        "\"\"\"\n",
        "Therefore, our ANN model predicts that this customer stays in the bank!\n",
        "Important note 1: Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n",
        "Important note 2: Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns.\n",
        "\"\"\"\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = ann.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Re5In5j6R-J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "\n",
        "Additional Reading:\n",
        "\n",
        "*   Additional Reading:\n",
        "\n",
        "    Yann LeCun et al., 1998, [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n",
        "\n",
        "    Jianxin Wu, 2017, [Introduction to Convolutional Neural Networks](http://cs.nju.edu.cn/wujx/paper/CNN.pdf)\n",
        "\n",
        "    C.-C. Jay Kuo, 2016, [Understanding Convolutional Neural Networks with A Mathematical Model](https://arxiv.org/pdf/1609.04112.pdf)\n",
        "\n",
        "    Kaiming He et al., 2015, [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf)\n",
        "\n",
        "    Dominik Scherer et al., 2010, [Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf)\n",
        "\n",
        "    Adit Deshpande, 2016, [The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html/)\n",
        "\n",
        "    Rob DiPietro, 2016, [A Friendly Introduction to Cross-Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)\n",
        "\n",
        "    Peter Roelants, 2016, [How to implement a neural network Intermezzo 2](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zi8g-PdOU6PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convolutional Neural Network\n",
        "\n",
        "# Importing the libraries\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "tf.__version__\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Preprocessing the Training set\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "# Preprocessing the Test set\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "# Part 2 - Building the CNN\n",
        "\n",
        "# Initialising the CNN\n",
        "cnn = tf.keras.models.Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Step 4 - Full Connection\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# Step 5 - Output Layer\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Part 3 - Training the CNN\n",
        "\n",
        "# Compiling the CNN\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Training the CNN on the Training set and evaluating it on the Test set\n",
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n",
        "\n",
        "# Part 4 - Making a single prediction\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "hr-iuOYkR-Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Network\n",
        "\n",
        "*   Additional Reading:\n",
        "\n",
        "    Oscar Sharp & Benjamin, 2016, [Sunspring](https://arstechnica.com/the-multiverse/2016/06/an-ai-wrote-this-movie-and-its-strangely-moving/)\n",
        "\n",
        "    Sepp (Josef) Hochreiter, 1991, [Untersuchungen zu dynamischen neuronalen Netzen](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf)\n",
        "\n",
        "    Yoshua Bengio, 1994, [Learning Long-Term Dependencies with Gradient Descent is Difficult](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)\n",
        "\n",
        "    Razvan Pascanu, 2013, [On the difficulty of training recurrent neural networks](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf)\n",
        "\n",
        "    Sepp Hochreiter & Jurgen Schmidhuber, 1997, [Long Short-Term Memory](http://www.bioinf.jku.at/publications/older/2604.pdf) \n",
        "\n",
        "    Christopher Olah, 2015, [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "    Shi Yan, 2016, [Understanding LSTM and its diagrams](https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714)\n",
        "\n",
        "    Andrej Karpathy, 2015, [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "    Andrej Karpathy, 2015, [Visualizing and Understanding Recurrent Networks](https://arxiv.org/pdf/1506.02078.pdf)\n",
        "\n",
        "    Klaus Greff, 2015, [LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf)\n",
        "\n",
        "    Xavier Glorot, 2011, [Deep sparse rectifier neural networks](http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "N2x5chz6Vik2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the training set\n",
        "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')\n",
        "training_set = dataset_train.iloc[:, 1:2].values\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        "\n",
        "# Creating a data structure with 60 timesteps and 1 output\n",
        "X_train = []\n",
        "y_train = []\n",
        "for i in range(60, 1258):\n",
        "    X_train.append(training_set_scaled[i-60:i, 0])\n",
        "    y_train.append(training_set_scaled[i, 0])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "# Reshaping\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "\n",
        "\n",
        "# Part 2 - Building the RNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Initialising the RNN\n",
        "regressor = Sequential()\n",
        "\n",
        "# Adding the first LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding a second LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding a third LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding a fourth LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "regressor.add(Dense(units = 1))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "# Fitting the RNN to the Training set\n",
        "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
        "\n",
        "\n",
        "\n",
        "# Part 3 - Making the predictions and visualising the results\n",
        "\n",
        "# Getting the real stock price of 2017\n",
        "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
        "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
        "\n",
        "# Getting the predicted stock price of 2017\n",
        "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
        "inputs = inputs.reshape(-1,1)\n",
        "inputs = sc.transform(inputs)\n",
        "X_test = []\n",
        "for i in range(60, 80):\n",
        "    X_test.append(inputs[i-60:i, 0])\n",
        "X_test = np.array(X_test)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "predicted_stock_price = regressor.predict(X_test)\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
        "\n",
        "# Visualising the results\n",
        "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
        "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "40TmOTsFR-P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Organizing Map\n",
        "\n",
        "\n",
        "*   Additional Reading:\n",
        "\n",
        "    Tuevo Kohonen, 1990, [The Self-Organizing Map](http://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf)\n",
        "\n",
        "    Mat Buckland, 2004?, [Kohonen's Self Organizing Feature Maps](http://www.ai-junkie.com/ann/som/som1.html)\n",
        "    \n",
        "    Nadieh Bremer, 2003, [SOM – Creating hexagonal heatmaps with D3.js](https://www.visualcinnamon.com/2013/07/self-organizing-maps-creating-hexagonal.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IaUPwqE9WGAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Install MiniSom Package\n",
        "\"\"\"\n",
        "\n",
        "!pip install MiniSom\n",
        "\n",
        "\"\"\"### Importing the libraries\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"## Importing the dataset\"\"\"\n",
        "\n",
        "dataset = pd.read_csv('Credit_Card_Applications.csv')\n",
        "X = dataset.iloc[:, :-1].values \n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "\"\"\"## Feature Scaling\"\"\"\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0,1))\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "\"\"\"##Training the SOM\"\"\"\n",
        "\n",
        "from minisom import MiniSom\n",
        "som = MiniSom(x=10, y=10, input_len= 15, sigma= 1.0, learning_rate = 0.5)\n",
        "som.random_weights_init(X)\n",
        "som.train_random(data = X, num_iteration = 100)\n",
        "\n",
        "\"\"\"##Visualizing the results\"\"\"\n",
        "\n",
        "from pylab import bone, pcolor, colorbar, plot, show\n",
        "bone()\n",
        "pcolor(som.distance_map().T)\n",
        "colorbar()\n",
        "markers = ['o', 's']\n",
        "colors = ['r', 'g']\n",
        "for i, x in enumerate(X):\n",
        "    w = som.winner(x)\n",
        "    plot(w[0] + 0.5,\n",
        "         w[1] + 0.5,\n",
        "         markers[y[i]],\n",
        "         markeredgecolor = colors[y[i]],\n",
        "         markerfacecolor = 'None',\n",
        "         markersize = 10,\n",
        "         markeredgewidth = 2)\n",
        "show()\n",
        "\n",
        "\"\"\"## Finding the frauds\"\"\"\n",
        "\n",
        "mappings = som.win_map(X)\n",
        "frauds = np.concatenate((mappings[(1,1)], mappings[(4,1)]), axis = 0)\n",
        "frauds = sc.inverse_transform(frauds)\n",
        "\n",
        "\"\"\"##Printing the Fraunch Clients\"\"\"\n",
        "\n",
        "print('Fraud Customer IDs')\n",
        "for i in frauds[:, 0]:\n",
        "  print(int(i))"
      ],
      "metadata": {
        "id": "mkNaNO1iR-SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boltzmann Machine\n",
        "\n",
        "\n",
        "\n",
        "*   Additional Reading:\n",
        "\n",
        "    Yann LeCun, 2006, [A Tutorial on Energy-Based Learning](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)\n",
        "\n",
        "    Jaco Van Dormael, 2009, [Mr. Nobody](http://www.imdb.com/title/tt0485947/)\n",
        "\n",
        "    Geoffrey Hinton, 2006, [A fast learning algorithm for deep belief nets](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)\n",
        "\n",
        "    Oliver Woodford, 2012?, [Notes on Contrastive Divergence](http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf)\n",
        "\n",
        " Yoshua Bengio, 2006, [Greedy Layer-Wise Training of Deep Networks](http://www.iro.umontreal.ca/~lisa/pointeurs/BengioNips2006All.pdf)\n",
        "\n",
        "    Geoffrey Hinton, 1995, [The wake-sleep algorithm for unsupervised neural networks](http://www.gatsby.ucl.ac.uk/~dayan/papers/hdfn95.pdf)\n",
        "\n",
        "    Ruslan Salakhutdinov, 2009?, [Deep Boltzmann Machines](http://www.utstat.toronto.edu/~rsalakhu/papers/dbm.pdf)\n",
        "    "
      ],
      "metadata": {
        "id": "X9gHXzrpXHUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the dataset\n",
        "\n",
        "###ML-100K\n",
        "\"\"\"\n",
        "\n",
        "!wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "!unzip ml-100k.zip\n",
        "!ls\n",
        "\n",
        "\"\"\"###ML-1M\"\"\"\n",
        "\n",
        "!wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
        "!unzip ml-1m.zip\n",
        "!ls\n",
        "\n",
        "\"\"\"##Importing the libraries\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\"\"\"## Importing the dataset\"\"\"\n",
        "\n",
        "# We won't be using this dataset.\n",
        "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "\"\"\"## Preparing the training set and the test set\"\"\"\n",
        "\n",
        "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n",
        "test_set = np.array(test_set, dtype = 'int')\n",
        "\n",
        "\"\"\"## Getting the number of users and movies\"\"\"\n",
        "\n",
        "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
        "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))\n",
        "\n",
        "\"\"\"## Converting the data into an array with users in lines and movies in columns\"\"\"\n",
        "\n",
        "def convert(data):\n",
        "  new_data = []\n",
        "  for id_users in range(1, nb_users + 1):\n",
        "    id_movies = data[:, 1] [data[:, 0] == id_users]\n",
        "    id_ratings = data[:, 2] [data[:, 0] == id_users]\n",
        "    ratings = np.zeros(nb_movies)\n",
        "    ratings[id_movies - 1] = id_ratings\n",
        "    new_data.append(list(ratings))\n",
        "  return new_data\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)\n",
        "\n",
        "\"\"\"## Converting the data into Torch tensors\"\"\"\n",
        "\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)\n",
        "\n",
        "\"\"\"## Converting the ratings into binary ratings 1 (Liked) or 0 (Not Liked)\"\"\"\n",
        "\n",
        "training_set[training_set == 0] = -1\n",
        "training_set[training_set == 1] = 0\n",
        "training_set[training_set == 2] = 0\n",
        "training_set[training_set >= 3] = 1\n",
        "test_set[test_set == 0] = -1\n",
        "test_set[test_set == 1] = 0\n",
        "test_set[test_set == 2] = 0\n",
        "test_set[test_set >= 3] = 1\n",
        "\n",
        "\"\"\"## Creating the architecture of the Neural Network\"\"\"\n",
        "\n",
        "class RBM():\n",
        "  def __init__(self, nv, nh):\n",
        "    self.W = torch.randn(nh, nv)\n",
        "    self.a = torch.randn(1, nh)\n",
        "    self.b = torch.randn(1, nv)\n",
        "  def sample_h(self, x):\n",
        "    wx = torch.mm(x, self.W.t())\n",
        "    activation = wx + self.a.expand_as(wx)\n",
        "    p_h_given_v = torch.sigmoid(activation)\n",
        "    return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
        "  def sample_v(self, y):\n",
        "    wy = torch.mm(y, self.W)\n",
        "    activation = wy + self.b.expand_as(wy)\n",
        "    p_v_given_h = torch.sigmoid(activation)\n",
        "    return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
        "  def train(self, v0, vk, ph0, phk):\n",
        "    self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n",
        "    self.b += torch.sum((v0 - vk), 0)\n",
        "    self.a += torch.sum((ph0 - phk), 0)\n",
        "nv = len(training_set[0])\n",
        "nh = 100\n",
        "batch_size = 100\n",
        "rbm = RBM(nv, nh)\n",
        "\n",
        "\"\"\"## Training the RBM\"\"\"\n",
        "\n",
        "nb_epoch = 10\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "  train_loss = 0\n",
        "  s = 0.\n",
        "  for id_user in range(0, nb_users - batch_size, batch_size):\n",
        "    vk = training_set[id_user : id_user + batch_size]\n",
        "    v0 = training_set[id_user : id_user + batch_size]\n",
        "    ph0,_ = rbm.sample_h(v0)\n",
        "    for k in range(10):\n",
        "      _,hk = rbm.sample_h(vk)\n",
        "      _,vk = rbm.sample_v(hk)\n",
        "      vk[v0<0] = v0[v0<0]\n",
        "    phk,_ = rbm.sample_h(vk)\n",
        "    rbm.train(v0, vk, ph0, phk)\n",
        "    train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0]))\n",
        "    s += 1.\n",
        "  print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n",
        "\n",
        "\"\"\"## Testing the RBM\"\"\"\n",
        "\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    v = training_set[id_user:id_user+1]\n",
        "    vt = test_set[id_user:id_user+1]\n",
        "    if len(vt[vt>=0]) > 0:\n",
        "        _,h = rbm.sample_h(v)\n",
        "        _,v = rbm.sample_v(h)\n",
        "        test_loss += torch.mean(torch.abs(vt[vt>=0] - v[vt>=0]))\n",
        "        s += 1.\n",
        "print('test loss: '+str(test_loss/s))"
      ],
      "metadata": {
        "id": "C4jXkxd8R-V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoEncoders\n",
        "\n",
        "*   Additional Reading:\n",
        "\n",
        "    Malte Skarupke, 2016, [Neural Networks Are Impressively Good At Compression](https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression)\n",
        "\n",
        "    Francois Chollet, 2016, [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
        "\n",
        "    Chris McCormick, 2014, Deep Learning Tutorial - [Sparse Autoencoder](http://mccormickml.com/2014/05/30/deep-learning-tutorial-sparse-autoencoder)\n",
        "\n",
        "    Eric Wilkinson, 2014, [Deep Learning: Sparse Autoencoders](http://www.ericlwilkinson.com/blog/2014/11/19/deep-learning-sparse-autoencoders)\n",
        "\n",
        "    Alireza Makhzani, 2014, [k-Sparse Autoencoders](https://arxiv.org/pdf/1312.5663.pdf)\n",
        "\n",
        "    Pascal Vincent, 2008, [Extracting and Composing Robust Features with Denoising Autoencoders](http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf)\n",
        "\n",
        "    Salah Rifai, 2011, [Contractive Auto-Encoders: Explicit Invariance During Feature Extraction](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf)\n",
        "\n",
        "    Pascal Vincent, 2010, [Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion](http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf)\n",
        "\n",
        "    Geoffrey Hinton, 2006, [Reducing the Dimensionality of Data with Neural Networks](https://www.cs.toronto.edu/~hinton/science.pdf)\n",
        "\n"
      ],
      "metadata": {
        "id": "X2ThSy_sYBVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Downloading the dataset\n",
        "\n",
        "###ML-100K\n",
        "\"\"\"\n",
        "\n",
        "!wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "!unzip ml-100k.zip\n",
        "!ls\n",
        "\n",
        "\"\"\"###ML-1M\"\"\"\n",
        "\n",
        "!wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
        "!unzip ml-1m.zip\n",
        "!ls\n",
        "\n",
        "\"\"\"##Importing the libraries\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\"\"\"## Importing the dataset\"\"\"\n",
        "\n",
        "# We won't be using this dataset.\n",
        "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "\"\"\"## Preparing the training set and the test set\"\"\"\n",
        "\n",
        "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n",
        "test_set = np.array(test_set, dtype = 'int')\n",
        "\n",
        "\"\"\"## Getting the number of users and movies\"\"\"\n",
        "\n",
        "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
        "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))\n",
        "\n",
        "\"\"\"## Converting the data into an array with users in lines and movies in columns\"\"\"\n",
        "\n",
        "def convert(data):\n",
        "  new_data = []\n",
        "  for id_users in range(1, nb_users + 1):\n",
        "    id_movies = data[:, 1] [data[:, 0] == id_users]\n",
        "    id_ratings = data[:, 2] [data[:, 0] == id_users]\n",
        "    ratings = np.zeros(nb_movies)\n",
        "    ratings[id_movies - 1] = id_ratings\n",
        "    new_data.append(list(ratings))\n",
        "  return new_data\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)\n",
        "\n",
        "\"\"\"## Converting the data into Torch tensors\"\"\"\n",
        "\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)\n",
        "\n",
        "\"\"\"## Creating the architecture of the Neural Network\"\"\"\n",
        "\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)\n",
        "\n",
        "\"\"\"## Training the SAE\"\"\"\n",
        "\n",
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "  train_loss = 0\n",
        "  s = 0.\n",
        "  for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = input.clone()\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "      output = sae(input)\n",
        "      target.require_grad = False\n",
        "      output[target == 0] = 0\n",
        "      loss = criterion(output, target)\n",
        "      mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "      loss.backward()\n",
        "      train_loss += np.sqrt(loss.data*mean_corrector)\n",
        "      s += 1.\n",
        "      optimizer.step()\n",
        "  print('epoch: '+str(epoch)+'loss: '+ str(train_loss/s))\n",
        "\n",
        "\"\"\"## Testing the SAE\"\"\"\n",
        "\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "  input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "  target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "  if torch.sum(target.data > 0) > 0:\n",
        "    output = sae(input)\n",
        "    target.require_grad = False\n",
        "    output[target == 0] = 0\n",
        "    loss = criterion(output, target)\n",
        "    mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "    test_loss += np.sqrt(loss.data*mean_corrector)\n",
        "    s += 1.\n",
        "print('test loss: '+str(test_loss/s))"
      ],
      "metadata": {
        "id": "tDV5Q3g8WzVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}